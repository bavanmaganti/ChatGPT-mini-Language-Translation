{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3Ri3nwGiqbkp0C9tzudPK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Gathering Data\n",
        "\n"
      ],
      "metadata": {
        "id": "ypW94HuOq_Ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is the cornerstone of any machine learning project. For our translation model, we'll use the French-English dataset from TensorFlow's official repository. We'll download and extract the data using TensorFlow's utility functions."
      ],
      "metadata": {
        "id": "DDzA7Z6WsX63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Gathering data using TensorFlow's utility function\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname='fra-eng.zip',\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "vvOrRk2ErQ-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing"
      ],
      "metadata": {
        "id": "5nXgmjS-sxig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the data, the next step is preprocessing. We'll normalize the text, handle special characters, and format it appropriately for training."
      ],
      "metadata": {
        "id": "AQnbr17-s4a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pathlib\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "# Defining the path to the text file\n",
        "text_file = pathlib.Path(text_file).parent / 'fra.txt'\n",
        "\n",
        "def normalize(line):\n",
        "    # Normalize unicode characters, strip leading/trailing whitespace, convert to lowercase\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
        "    # Handle special characters and add start and end tokens for the target language (French)\n",
        "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1\", line)\n",
        "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1\", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\"\\1\", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\"\\1\", line)\n",
        "    eng, fre = line.split(\"\\t\")\n",
        "    fre = '[start] ' + fre + ' [end]'\n",
        "    return eng, fre\n",
        "\n",
        "# Read and normalize the text pairs\n",
        "with open(text_file) as fp:\n",
        "    text_pairs = [normalize(line) for line in fp]\n",
        "\n"
      ],
      "metadata": {
        "id": "355j57lDs6bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization and Statistics"
      ],
      "metadata": {
        "id": "nVHXScZJtArM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training, it's crucial to tokenize our text data and understand its characteristics, such as the vocabulary size and maximum sequence lengths."
      ],
      "metadata": {
        "id": "fqxKKPRYtEI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Statistics\n",
        "\n",
        "# Initialize sets to store unique tokens for English and French\n",
        "eng_tokens, fre_tokens = set(), set()\n",
        "# Initialize variables to store maximum sequence lengths\n",
        "eng_maxlen, fre_maxlen = 0, 0\n",
        "\n",
        "# Iterate through text pairs to tokenize and compute statistics\n",
        "for eng, fre in text_pairs:\n",
        "    eng_token, fre_token = eng.split(), fre.split()\n",
        "    eng_maxlen = max(eng_maxlen, len(eng_token))\n",
        "    fre_maxlen = max(fre_maxlen, len(fre_token))\n",
        "    eng_tokens.update(eng_token)\n",
        "    fre_tokens.update(fre_token)\n",
        "\n",
        "# Print statistics\n",
        "print(f\"Total tokens in English: {len(eng_tokens)}\")\n",
        "print(f\"Total tokens in French: {len(fre_tokens)}\")\n",
        "print(f\"Maximum length of English sequence: {eng_maxlen}\")\n",
        "print(f\"Maximum length of French sequence: {fre_maxlen}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "752hpM3ZtGLS",
        "outputId": "81c17548-4bbd-4f91-dbec-0b4b3b541cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens in English: 25365\n",
            "Total tokens in French: 42027\n",
            "Maximum length of English sequence: 47\n",
            "Maximum length of French sequence: 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Serialization"
      ],
      "metadata": {
        "id": "7e1moW2btLwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we'll serialize our preprocessed data for future use."
      ],
      "metadata": {
        "id": "5oD4-DuWtQGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Serialize preprocessed data for future use\n",
        "with open(\"text_pairs.pickle\", 'wb') as fp:\n",
        "    pickle.dump(text_pairs, fp)\n"
      ],
      "metadata": {
        "id": "m1GGbgL1tSGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding Layer"
      ],
      "metadata": {
        "id": "1tOm_p1wtbAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step in building our model is data transformation. We'll preprocess our text data, vectorize it, and create TensorFlow datasets for training and testing.\n"
      ],
      "metadata": {
        "id": "9Mj1KlGvtp-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Load preprocessed text pairs\n",
        "with open(\"text_pairs.pickle\", 'rb') as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# Shuffle the data\n",
        "random.shuffle(text_pairs)\n",
        "\n",
        "# Split into train and test sets\n",
        "n_val = int(0.15 * len(text_pairs))\n",
        "n_train = len(text_pairs) - 2 * n_val\n",
        "train_pair = text_pairs[:n_train]\n",
        "test_pair = text_pairs[n_train: n_train + n_val]\n",
        "\n",
        "# Vocabulary sizes and sequence length\n",
        "vocab_en = 10000\n",
        "vocab_fr = 20000\n",
        "seq_length = 25\n",
        "\n",
        "# Initialize TextVectorization layers\n",
        "eng_vect = TextVectorization(\n",
        "    max_tokens=vocab_en,\n",
        "    standardize=None,\n",
        "    split='whitespace',\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_length\n",
        ")\n",
        "\n",
        "fre_vect = TextVectorization(\n",
        "    max_tokens=vocab_fr,\n",
        "    standardize=None,\n",
        "    split='whitespace',\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_length + 1  # +1 for start token\n",
        ")\n",
        "\n",
        "# Adapt TextVectorization layers to training data\n",
        "train_eng = [pair[0] for pair in train_pair]\n",
        "train_fre = [pair[1] for pair in train_pair]\n",
        "\n",
        "eng_vect.adapt(train_eng)\n",
        "fre_vect.adapt(train_fre)\n",
        "\n",
        "\n",
        "# Serialize the vectorization layers and training/test data\n",
        "with open('vectorize.pickle', 'wb') as fp:\n",
        "    data = {'train': train_pair,\n",
        "            'test': test_pair,\n",
        "            'eng_vect': eng_vect.get_config(),\n",
        "            'fre_vect': fre_vect.get_config(),\n",
        "            'eng_weights': eng_vect.get_weights(),\n",
        "            'fre_weights': fre_vect.get_weights()\n",
        "            }\n",
        "    pickle.dump(data, fp)\n",
        "\n",
        "# Load serialized data\n",
        "with open(\"vectorize.pickle\", 'rb') as fp:\n",
        "    data = pickle.load(fp)\n",
        "\n",
        "# Retrieve train and test pairs\n",
        "train_pair = data['train']\n",
        "test_pair = data['test']\n",
        "\n",
        "# Reconstruct TextVectorization layers\n",
        "eng_vect = TextVectorization.from_config(data['eng_vect'])\n",
        "eng_vect.set_weights(data['eng_weights'])\n",
        "fre_vect = TextVectorization.from_config(data['fre_vect'])\n",
        "fre_vect.set_weights(data['fre_weights'])\n",
        "\n",
        "# Define function to format dataset\n",
        "def format_dataset(eng, fre):\n",
        "    eng = eng_vect(eng)\n",
        "    fre = fre_vect(fre)\n",
        "    source = {'encode_inp': eng,\n",
        "              'decode_inp': fre[:, :-1]\n",
        "              }\n",
        "    target = fre[:, 1:]\n",
        "    return (source, target)\n",
        "\n",
        "# Define function to create dataset\n",
        "def make_dataset(pairs, batchsize=64):\n",
        "    eng_text, fre_text = zip(*pairs)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_text), list(fre_text)))\n",
        "    return dataset.shuffle(2048).batch(batchsize).map(format_dataset).prefetch(16).cache()\n",
        "\n",
        "# Create TensorFlow datasets for training and testing\n",
        "train_ds = make_dataset(train_pair)\n",
        "test_ds = make_dataset(test_pair)\n"
      ],
      "metadata": {
        "id": "5SDUdYust0VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Positional Embedding"
      ],
      "metadata": {
        "id": "9dpABUN-t7wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's implement the positional embedding layer. This layer is essential for helping the model understand the sequential order of tokens in a sentence."
      ],
      "metadata": {
        "id": "ktr2sl5cuB3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Function to generate positional encoding matrix\n",
        "def pos_enc_matrix(L, d, n=10000):\n",
        "    assert d % 2 == 0\n",
        "    d2 = d // 2\n",
        "\n",
        "    P = np.zeros((L, d))\n",
        "    k = np.arange(L).reshape(-1, 1)\n",
        "    i = np.arange(d2).reshape(1, -1)\n",
        "\n",
        "    denom = np.power(n, -i / d2)\n",
        "    args = k * denom\n",
        "\n",
        "    P[:, ::2] = np.sin(args)\n",
        "    P[:, 1::2] = np.cos(args)\n",
        "    return P\n",
        "\n",
        "# Custom Keras layer for positional embedding\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, seq_length, vocab_size, embed_dim, **kwargs):\n",
        "      super().__init__(**kwargs)\n",
        "      self.seq_length = seq_length\n",
        "      self.vocab_size = vocab_size\n",
        "      self.embed_dim = embed_dim\n",
        "\n",
        "      self.token_embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
        "      matrix = pos_enc_matrix(seq_length, embed_dim)\n",
        "\n",
        "      self.positional_embedding = tf.constant(matrix, dtype='float32')\n",
        "\n",
        "  def call(self, inputs):\n",
        "      embedded_tokens = self.token_embeddings(inputs)\n",
        "      return embedded_tokens + self.positional_embedding\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "      return self.token_embeddings.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def get_config(self):\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "          'seq_length': self.seq_length,\n",
        "          'vocab_size': self.vocab_size,\n",
        "          'embed_dim': self.embed_dim\n",
        "      })\n",
        "\n",
        "# Usage and Validation\n",
        "vocab_en = 10000\n",
        "seq_length = 25\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    embed_en = PositionalEmbedding(seq_length, vocab_en, embed_dim=512)\n",
        "    en_emb = embed_en(inputs['encode_inp'])\n",
        "    print(en_emb._keras_mask)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Hyethu6ZzVPR",
        "outputId": "53b2d484-c579-414f-c438-6e9428d0b678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to MapDataset:55 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::MemoryCacheImpl::Prefetch::Map: Table not initialized.\n\t [[{{node text_vectorization_7_1/None_Lookup/LookupTableFindV2}}]] [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-0b0abda1f5b4>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0membed_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionalEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0men_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encode_inp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5982\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5983\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to MapDataset:55 transformation with iterator: Iterator::Root::Prefetch::FiniteTake::MemoryCacheImpl::Prefetch::Map: Table not initialized.\n\t [[{{node text_vectorization_7_1/None_Lookup/LookupTableFindV2}}]] [Op:IteratorGetNext] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Self-Attention Layer\n",
        "The self-attention mechanism allows our model to weigh the importance of different words within the same input sequence. Let's define a function to create a self-attention layer.\n"
      ],
      "metadata": {
        "id": "aPvJMX8M5Id1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def self_attention(input_shape, prefix='att', mask=False, **kwargs):\n",
        "    # Define inputs\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f\"{prefix}_in1\")\n",
        "\n",
        "    # Multi-head attention layer\n",
        "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_att1\", **kwargs)\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f'{prefix}_norm1')\n",
        "    add = tf.keras.layers.Add(name=f'{prefix}_add1')\n",
        "\n",
        "    # Apply attention mechanism\n",
        "    attout = attention(query=inputs, value=inputs, key=inputs, use_causal_mask=mask)\n",
        "\n",
        "    # Apply normalization and residual connection\n",
        "    output = norm(add([inputs, attout]))\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=output, name=f\"{prefix}_att\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "G-YlTbXs5UXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cross-Attention Layer\n",
        "\n",
        "The cross-attention layer enables our model to focus on relevant information from an external context, such as the source sentence in translation tasks. Let's define a function to create a cross-attention layer."
      ],
      "metadata": {
        "id": "dK8AuVaA5YGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_attention(input_shape, context_shape, prefix='att', **kwargs):\n",
        "    # Define inputs\n",
        "    context = tf.keras.layers.Input(shape=context_shape, dtype='float32', name=f\"{prefix}_ctx2\")\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in2')\n",
        "\n",
        "    # Multi-head attention layer\n",
        "    attention = tf.keras.layers.MultiHeadAttention(name=f'{prefix}_att2', **kwargs)\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f'{prefix}_norm2')\n",
        "    add = tf.keras.layers.Add(name=f'{prefix}_add2')\n",
        "\n",
        "    # Apply attention mechanism\n",
        "    attout = attention(query=inputs, key=context, value=context)\n",
        "\n",
        "    # Apply normalization and residual connection\n",
        "    output = norm(add([attout, inputs]))\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=[context, inputs], outputs=output, name=f'{prefix}_crs_at')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Ghu6xpVz5bxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feed-Forward Layer\n",
        "\n",
        "The feed-forward layer consists of two dense layers with a ReLU activation function, followed by dropout and layer normalization. Let's define a function to create the feed-forward layer."
      ],
      "metadata": {
        "id": "YLJydU8L5dwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward(input_shape, model_dim, ff_dim, dropout=.1, prefix='ff'):\n",
        "    # Define inputs\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in3')\n",
        "\n",
        "    # Dense layers\n",
        "    dense1 = tf.keras.layers.Dense(ff_dim, name=f'{prefix}_ff1', activation='relu')\n",
        "    dense2 = tf.keras.layers.Dense(model_dim, name=f'{prefix}_ff2')\n",
        "    drop = tf.keras.layers.Dropout(dropout, name=f'{prefix}_drop')\n",
        "    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n",
        "\n",
        "    # Apply feed-forward transformation\n",
        "    ffout = drop(dense2(dense1(inputs)))\n",
        "\n",
        "    # Layer normalization and residual connection\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f'{prefix}_norm3')\n",
        "    output = norm(add([inputs, ffout]))\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=output, name=f'{prefix}_ff')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "8wv40NB85j2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder Layer\n",
        "\n",
        "The encoder processes the input sequence and extracts meaningful representations. It consists of self-attention and feed-forward layers."
      ],
      "metadata": {
        "id": "-ZiOuPDd56KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def encoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix='enc', **kwargs):\n",
        "    # Define a Sequential model for the encoder\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in0'), # Input layer\n",
        "        self_attention(input_shape, prefix=prefix, key_dim=key_dim, mask=False, **kwargs), # Self-attention layer\n",
        "        feed_forward(input_shape, key_dim, ff_dim, dropout, prefix) # Feed-forward layer\n",
        "    ])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "NPyqqy3P5-dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoder Layer\n",
        "\n",
        "The decoder takes the encoder's output and generates the translated sequence. It comprises self-attention, cross-attention, and feed-forward layers."
      ],
      "metadata": {
        "id": "Dq6iqNcr6Avz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix='dec', **kwargs):\n",
        "    # Define inputs for decoder\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_in0')\n",
        "    context = tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f'{prefix}_ctx0')\n",
        "\n",
        "    # Self-attention and cross-attention layers\n",
        "    att_model = self_attention(input_shape, key_dim=key_dim, mask=True, prefix=prefix, **kwargs)\n",
        "    cross_model = cross_attention(input_shape, input_shape, key_dim=key_dim, prefix=prefix, **kwargs)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ff_model = feed_forward(input_shape, key_dim, ff_dim, dropout, prefix)\n",
        "\n",
        "    # Connect layers\n",
        "    x = att_model(inputs)\n",
        "    x = cross_model([context, x])\n",
        "    output = ff_model(x)\n",
        "\n",
        "    # Define decoder model\n",
        "    model = tf.keras.Model(inputs=[inputs, context], outputs=output, name=prefix)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "BDViDkNz6HMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Model\n",
        "\n",
        "The transformer model combines the encoder and decoder to perform language translation."
      ],
      "metadata": {
        "id": "Q-WhmUWN6Imi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(num_layers, num_heads, seq_length, key_dim, ff_dim, vocab_size_en, vocab_size_fr, dropout=0.1, name='transformer'):\n",
        "    # Define encoder and decoder inputs\n",
        "    input_enc = tf.keras.layers.Input(shape=(seq_length), dtype='int32', name='encode_inp')\n",
        "    input_dec = tf.keras.layers.Input(shape=(seq_length), dtype='int32', name='decode_inp')\n",
        "\n",
        "    # Positional embeddings for encoder and decoder inputs\n",
        "    emb_enc = PositionalEmbedding(seq_length, vocab_size_en, key_dim, name='embed_enc')\n",
        "    emb_dec = PositionalEmbedding(seq_length, vocab_size_fr, key_dim, name='embed_dec')\n",
        "\n",
        "    # Create encoder and decoder layers\n",
        "    encoders = [encoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim, dropout=dropout, prefix=f\"enc{i}\", num_heads=num_heads)\n",
        "                for i in range(num_layers)]\n",
        "    decoders = [decoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim, dropout=dropout, prefix=f\"dec{i}\", num_heads=num_heads)\n",
        "                for i in range(num_layers)]\n",
        "\n",
        "    # Final dense layer\n",
        "    final = tf.keras.layers.Dense(vocab_size_fr, name='linear')\n",
        "\n",
        "    # Apply encoder and decoder layers to inputs\n",
        "    x1 = emb_enc(input_enc)\n",
        "    x2 = emb_dec(input_dec)\n",
        "    for layer in encoders:\n",
        "        x1 = layer(x1)\n",
        "    for layer in decoders:\n",
        "        x2 = layer([x2, x1])\n",
        "\n",
        "    # Generate output\n",
        "    output = final(x2)\n",
        "\n",
        "    try:\n",
        "        del output.keras_mask\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Define transformer model\n",
        "    model = tf.keras.Model(inputs=[input_enc, input_dec], outputs=output, name=name)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "X4mZ48dg6PsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Learning Rate Schedule\n",
        "A dynamic learning rate schedule can improve training by adjusting the learning rate based on the training progress. Here, we define a custom learning rate schedule that gradually increases the learning rate during the warm-up phase and then decreases it inversely proportional to the square root of the step number."
      ],
      "metadata": {
        "id": "8Uo5ETrJ6dTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, key_dim, warmup_steps=40000):\n",
        "    super().__init__()\n",
        "    self.key_dim = key_dim\n",
        "    self.warmup_steps = warmup_steps\n",
        "    self.d = tf.cast(self.key_dim, tf.float32)\n",
        "\n",
        "  def __call__(self, step):\n",
        "    # Convert step to float32\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    # Calculate learning rate schedule\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    return tf.math.rsqrt(self.d) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "  def get_config(self):\n",
        "    # Configuration for serialization\n",
        "    config ={\n",
        "      \"key_dim\": self.key_dim,\n",
        "      \"warmup_steps\": self.warmup_steps\n",
        "    }\n",
        "    return config\n",
        "\n",
        "# Define key dimension and create learning rate schedule\n",
        "key_dim = 128\n",
        "lr_schedule = CustomSchedule(key_dim)\n"
      ],
      "metadata": {
        "id": "rtskL97w6h0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Masked Loss Function\n",
        "To handle padded sequences during training, we define a masked loss function. This function calculates the loss only for non-padded tokens in the input sequences."
      ],
      "metadata": {
        "id": "m9oRbLuk6kv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(label, pred):\n",
        "  # Create mask for non-padded tokens\n",
        "  mask = label != 0\n",
        "\n",
        "  # Sparse categorical cross-entropy loss\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none'\n",
        "  )\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  # Apply mask to loss\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  # Compute average loss\n",
        "  loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "X0dLKYcR6nn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Masked Accuracy Metric\n",
        "Similarly, we define a masked accuracy metric to evaluate model performance while considering only non-padded tokens.\n"
      ],
      "metadata": {
        "id": "4987GnmD6p0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_accuracy(label, pred):\n",
        "  # Convert predictions to class labels\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "\n",
        "  # Calculate match between labels and predictions\n",
        "  match = label == pred\n",
        "\n",
        "  # Apply mask to match\n",
        "  mask = label != 0\n",
        "  match = match & mask\n",
        "\n",
        "  # Compute accuracy\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match) / tf.reduce_sum(mask)\n"
      ],
      "metadata": {
        "id": "ZirKwMCT6t2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compile and Train the Model\n",
        "Finally, we compile the model using the custom loss function and metrics and train it on the provided datasets."
      ],
      "metadata": {
        "id": "NWEiugdW6vse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with custom loss and metrics\n",
        "model.compile(loss=masked_loss, optimizer=optimizer, metrics=mask_accuracy)\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_ds, epochs=20, validation_data=test_ds)\n"
      ],
      "metadata": {
        "id": "xzlRQ5_N6ymL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizing Training History\n",
        "First, let's visualize the training history of our model to understand its performance over epochs."
      ],
      "metadata": {
        "id": "wyjIx7uj65NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizing Training History\n",
        "fig, axs = plt.subplots(2, figsize=(6, 8), sharex=True)\n",
        "fig.suptitle('Training history')\n",
        "x = list(range(1, 21))  # Assuming 20 epochs\n",
        "axs[0].plot(x, history.history[\"loss\"], alpha=0.5, label=\"loss\")\n",
        "axs[0].plot(x, history.history[\"val_loss\"], alpha=0.5, label=\"val_loss\")\n",
        "axs[0].set_ylabel(\"Loss\")\n",
        "axs[0].legend(loc=\"upper right\")\n",
        "axs[1].plot(x, history.history[\"masked_accuracy\"], alpha=0.5, label=\"mask_accuracy\")\n",
        "axs[1].plot(x, history.history[\"val_masked_accuracy\"], alpha=0.5, label=\"val_mask_accuracy\")\n",
        "axs[1].set_ylabel(\"Accuracy\")\n",
        "axs[1].set_xlabel(\"Epoch\")\n",
        "axs[1].legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "20zwOpdm67T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Testing\n",
        "Next, let's define a function to translate English sentences to French using our trained model and evaluate its performance on sample test cases."
      ],
      "metadata": {
        "id": "whSp7z6S6-DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    # Encode input sentence\n",
        "    enc_tokens = eng_vect([sentence])\n",
        "    lookup = list(fra_vect.get_vocabulary())\n",
        "    start_sent, end_sent = \"[start]\", \"[end]\"\n",
        "    output_sent = [start_sent]\n",
        "    for i in range(seq_length):\n",
        "        # Prepare decoder input\n",
        "        vector = fra_vect([\" \".join(output_sent)])\n",
        "        assert vector.shape == (1, seq_length + 1)\n",
        "        dec_tokens = vector[:, :-1]\n",
        "        assert dec_tokens.shape == (1, seq_length)\n",
        "        # Generate predictions\n",
        "        pred = model([enc_tokens, dec_tokens])\n",
        "        assert pred.shape == (1, seq_len, vocab_size_fr)\n",
        "        # Decode predicted token\n",
        "        word = lookup[np.argmax(pred[0, i, :])]\n",
        "        output_sent.append(word)\n",
        "        if word == end_sent:\n",
        "            break\n",
        "    return output_sent\n",
        "\n",
        "# Test the model on sample test cases\n",
        "test_count = 20\n",
        "for n in range(test_count):\n",
        "    eng_sent, fre_sent = random.choice(test_pairs)\n",
        "    translated = translate(eng_sent)\n",
        "    print(f\"Test case: {n}\")\n",
        "    print(f\"English sentence: {eng_sent}\")\n",
        "    print(f\"Translated sentence: {' '.join(translated)}\")\n",
        "    print(f\"French sentence: {fre_sent}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "9FeC1t-m7CL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}